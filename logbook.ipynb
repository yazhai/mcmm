{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logbook and all things for Monte-Carlo Minimizor-Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "1. Monte-Carlo Descent method to search for optimal point\n",
    "  * Use a gradient-based local optimization method \n",
    "    * This requires auto-grad by Jax/PyTorch/TensorFlow.\n",
    "    * Use Jax for now.\n",
    "  * Use MCTD to balance exploration and exploitation of the search space\n",
    "2. Learn the lower bound of the objective function\n",
    "  * Use a quadratic model to approximate the lower bound of the objective function\n",
    "  * The learning is as a linear regression problem: finding `A`, such that `Ax <= y`\n",
    "3. Use interval computation to estimate if the optimal point is found within a small box\n",
    "  * Use ibex to compute the interval of the objective function \n",
    "4. Use branch-and-bound to split the search space into smaller boxes\n",
    "  * Better approx -> better pruning \n",
    "  * Prunning ambig box -> high priority "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems to solve\n",
    "\n",
    "Use an toy example  \n",
    "$\\min( 1000*(x+1)^2-3, (x-1)^2-1)$\n",
    "\n",
    "* During MCTD optimization, we have\n",
    "  * A list of samples from each trajectory\n",
    "  * A best-found point (x=1) from 1 or more nodes (but it is not the global min)\n",
    "  * 0 or more local best-found from 1 or more nodes\n",
    "\n",
    "3. Use interval computation to estimate if the optimal point is found within a small box  \n",
    "  * interval + function -> Ibex -> interval (can't tell directly when the formula is sat)\n",
    "    * Use IBEX to find a small box B' that $f == lb_f$ may occur in\n",
    "  * Better LB, better pruning?\n",
    "    * IBEX works better with quadratic instead of linear. \n",
    "    * The best lb to learn is a function that is NEITHER tightly lb to original function NOR completely flat\n",
    "  * Learned small box B', restart by another MCTD?\n",
    "    * No. Sample within the box for another node on MCTD\n",
    "    * We need a global LB for the original function, not a piecewise LB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "1. Synthetic functions\n",
    "  * [SFU](https://www.sfu.ca/~ssurjano/optimization.html)\n",
    "  * [Kyoto](http://www-optima.amp.i.kyoto-u.ac.jp/member/student/hedar/Hedar_files/TestGO_files/Page364.htm)\n",
    "\n",
    "2. Coconut Project (Maybe too old?)\n",
    "  * [Coconut](https://arnold-neumaier.at/glopt/coconut/Benchmark/Benchmark.html)\n",
    "\n",
    "3. Real Large Scale Global Optimization\n",
    "  * [CEC](https://github.com/P-N-Suganthan/2022-SO-BO/blob/main/CEC2022%20TR.pdf)\n",
    "\n",
    "4. Engineering problems\n",
    "  * [Matlab](https://www.mathworks.com/matlabcentral/fileexchange/124810-benchmark-problems?s_tid=FX_rc1_behav)\n",
    "\n",
    "5. Optimization of Black-Box Functions\n",
    "  * [Black-Box](https://github.com/christiangeissler/gradoptbenchmark/tree/master/)\n",
    "\n",
    "6. Sets of test problems for MINLP\n",
    "  * [MINLP](https://www.minlp.com/nlp-and-minlp-test-problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "1. [Gurobi]() - installed\n",
    "2. [Scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) - installed\n",
    "3. [PyGMO](https://esa.github.io/pagmo2/docs/cpp/algorithms/nlopt.html) - TODO \n",
    "4. [BARON](https://sahinidis.coe.gatech.edu/baron) - can do via [NEOS](https://neos-server.org/neos/) \n",
    "  * BARON guarantees to provide global optima under fairly general assumptions, but cannot handle constraints containing a goniometric function, an if-then-else statement or a reference to an external function. BARON requires that all nonlinear variables and expressions in the mathematical program are bounded from below and above. [source](https://documentation.aimms.com/platform/solvers/baron.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. Gradient based ? check if there are in `scipy.optimize` or `pygmo`\n",
    "5. Simulated Annealing ? check if there are in `scipy.optimize` or `pygmo`\n",
    "\n",
    "10. [Matlab FMinCon](https://www.mathworks.com/help/optim/ug/fmincon.html) - installed, but least priority because it is not free and not in python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jax]",
   "language": "python",
   "name": "conda-env-jax-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
