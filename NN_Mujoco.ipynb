{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e281e19-93fe-4dce-83ed-d3d27e4bfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.test_functions import NeuralNetworkOneLayer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".80\"\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06939f9-2661-4ee2-ba90-9cf6e8354040",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b067ed6-3095-4767-aff3-5ad688690330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "# sample_file = \"./InvertedDoublePendulum-v4_samples200000.pkl\"\n",
    "# save_name = \"InvertedDoublePendulum-v4\"\n",
    "# # Reward: 6.5 ~ 9.5, Err: 0.05\n",
    "\n",
    "# sample_file = \"./HalfCheetah-v4_samples2000000.pkl\"\n",
    "# save_name = \"HalfCheetah-v4\"\n",
    "# # Reward: -56 ~ 1.28  , Err: 50\n",
    "\n",
    "# sample_file = \"./Swimmer-v4_samples2000000.pkl\"\n",
    "# save_name = \"Swimmer-v4\"\n",
    "# # # Reward: -3.5 ~ 3.5, Err: 0.3\n",
    "\n",
    "# sample_file = \"Hopper-v4_samples2000000.pkl\"\n",
    "# save_name = \"Hopper-v4\"\n",
    "# Reward: -1.8746137439401072 , 3.90962180717704, Err: 0.21\n",
    "\n",
    "\n",
    "\n",
    "with open(sample_file, \"rb\") as fp:\n",
    "    sample_dict = pickle.load(fp)\n",
    "\n",
    "observations = sample_dict[\"observation\"]\n",
    "actions = sample_dict[\"actions\"]\n",
    "rewards = sample_dict[\"rewards\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1228b46-7e6b-4afd-892c-5bf7b9e10d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale the values into bounds\n",
    "BOUND = 9.9999\n",
    "\n",
    "\n",
    "def scale_to_bounds(lb, ub, bound):\n",
    "    scale = bound / np.maximum(np.abs(lb), np.abs(ub))\n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a596eb-98a2-4d91-b7fd-9552e9782dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds for actions are: lb = -9.999996642283573, ub = 9.999999824710532\n",
      "Bounds for observations are: lb = 0.0, ub = 9.9999\n",
      "Bounds after scaling are: lb = 0.0, ub = 9.9999\n"
     ]
    }
   ],
   "source": [
    "# Scale the observations/actions to uniform bounds\n",
    "lb, ub = np.min(actions), np.max(actions)\n",
    "print(f\"Bounds for actions are: lb = {lb}, ub = {ub}\")\n",
    "\n",
    "\n",
    "lb, ub = np.min(observations), np.max(observations)\n",
    "print(f\"Bounds for observations are: lb = {lb}, ub = {ub}\")\n",
    "scale = scale_to_bounds(lb, ub, BOUND)\n",
    "observations *= scale\n",
    "lb, ub = np.min(observations), np.max(observations)\n",
    "print(f\"Bounds after scaling are: lb = {lb}, ub = {ub}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b8ab7e-eaa2-4d20-b226-cf69fc080233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected samples are in the shape of:  (2000000, 14) (2000000,)\n",
      "Rewards range: -1.8746137439401072 , 3.90962180717704\n"
     ]
    }
   ],
   "source": [
    "# Create sample array\n",
    "xs = np.hstack((actions, observations))\n",
    "ys = -rewards\n",
    "\n",
    "print(\"Collected samples are in the shape of: \", xs.shape, ys.shape)\n",
    "print(f\"Rewards range: {np.min(rewards)} , {np.max(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e483f0-c036-4a4b-a3e9-e7914fb85372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73927ece-8450-4d25-b8c2-015344894ebe",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8012568e-d1af-467d-b410-1ad37b38cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "hidden_dims = 16\n",
    "num_epochs = 100\n",
    "batch_size = 1000\n",
    "learning_rate = 0.00001\n",
    "use_device = \"cuda:0\"  # \"cuda:0\" or \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2746814a-f92a-40bd-bee5-9cc1f05951ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup NN \n",
    "input_dims = xs.shape[-1]\n",
    "num_samples = xs.shape[0]\n",
    "bounds = np.array([-10.0, 10.0]*input_dims)\n",
    "\n",
    "\n",
    "# Create a neural network with one hidden layer\n",
    "nn = NeuralNetworkOneLayer(dims=input_dims, domain=bounds, hidden_dims=hidden_dims)\n",
    "\n",
    "criteria = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(nn.model.parameters(), lr=learning_rate)\n",
    "\n",
    "try:\n",
    "    device = torch.device(use_device)\n",
    "except:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7746ad75-fad2-4ffd-a673-420607421fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the neural network for 100 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d3517380a14ff5b319618c008a3491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yazhai/miniconda3/envs/jax/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 = 18.233884811401367\n",
      "Loss at step 1 = 2.335304021835327\n",
      "Loss at step 2 = 0.26897406578063965\n",
      "Loss at step 3 = 0.2274719774723053\n",
      "Loss at step 4 = 0.22084464132785797\n",
      "Loss at step 5 = 0.23096659779548645\n",
      "Loss at step 6 = 0.20581994950771332\n",
      "Loss at step 7 = 0.22670578956604004\n",
      "Loss at step 8 = 0.20735017955303192\n",
      "Loss at step 9 = 0.2381734848022461\n",
      "Loss at step 10 = 0.23744513094425201\n",
      "Loss at step 11 = 0.1987154334783554\n",
      "Loss at step 12 = 0.22855956852436066\n",
      "Loss at step 13 = 0.22944533824920654\n",
      "Loss at step 14 = 0.23685184121131897\n",
      "Loss at step 15 = 0.24826891720294952\n",
      "Loss at step 16 = 0.2204466015100479\n",
      "Loss at step 17 = 0.21390578150749207\n",
      "Loss at step 18 = 0.2000812441110611\n",
      "Loss at step 19 = 0.2198389619588852\n",
      "Loss at step 20 = 0.24096256494522095\n",
      "Loss at step 21 = 0.21327118575572968\n",
      "Loss at step 22 = 0.25725486874580383\n",
      "Loss at step 23 = 0.19167888164520264\n",
      "Loss at step 24 = 0.2197103500366211\n",
      "Loss at step 25 = 0.23100662231445312\n",
      "Loss at step 26 = 0.23339320719242096\n",
      "Loss at step 27 = 0.21452859044075012\n",
      "Loss at step 28 = 0.2035737782716751\n",
      "Loss at step 29 = 0.23564250767230988\n",
      "Loss at step 30 = 0.21604885160923004\n",
      "Loss at step 31 = 0.22206155955791473\n",
      "Loss at step 32 = 0.21744517982006073\n",
      "Loss at step 33 = 0.20679005980491638\n",
      "Loss at step 34 = 0.22002878785133362\n",
      "Loss at step 35 = 0.21998146176338196\n",
      "Loss at step 36 = 0.24588271975517273\n",
      "Loss at step 37 = 0.2195596843957901\n",
      "Loss at step 38 = 0.21305030584335327\n",
      "Loss at step 39 = 0.23630844056606293\n",
      "Loss at step 40 = 0.2158687561750412\n",
      "Loss at step 41 = 0.24209581315517426\n",
      "Loss at step 42 = 0.2292991280555725\n",
      "Loss at step 43 = 0.21209271252155304\n",
      "Loss at step 44 = 0.19995534420013428\n",
      "Loss at step 45 = 0.20468933880329132\n",
      "Loss at step 46 = 0.20940819382667542\n",
      "Loss at step 47 = 0.19925084710121155\n",
      "Loss at step 48 = 0.23394900560379028\n",
      "Loss at step 49 = 0.21537889540195465\n",
      "Loss at step 50 = 0.22840753197669983\n",
      "Loss at step 51 = 0.2208014279603958\n",
      "Loss at step 52 = 0.2221059799194336\n",
      "Loss at step 53 = 0.20599348843097687\n",
      "Loss at step 54 = 0.2267940640449524\n",
      "Loss at step 55 = 0.21259388327598572\n",
      "Loss at step 56 = 0.20103023946285248\n",
      "Loss at step 57 = 0.207988902926445\n",
      "Loss at step 58 = 0.20575079321861267\n",
      "Loss at step 59 = 0.20032502710819244\n",
      "Loss at step 60 = 0.22478796541690826\n",
      "Loss at step 61 = 0.22672885656356812\n",
      "Loss at step 62 = 0.22928614914417267\n",
      "Loss at step 63 = 0.2214784026145935\n",
      "Loss at step 64 = 0.216294065117836\n",
      "Loss at step 65 = 0.22861702740192413\n",
      "Loss at step 66 = 0.20905786752700806\n",
      "Loss at step 67 = 0.2313474416732788\n",
      "Loss at step 68 = 0.2188570648431778\n",
      "Loss at step 69 = 0.23105008900165558\n",
      "Loss at step 70 = 0.22130824625492096\n",
      "Loss at step 71 = 0.21071448922157288\n",
      "Loss at step 72 = 0.21514540910720825\n",
      "Loss at step 73 = 0.2059997171163559\n",
      "Loss at step 74 = 0.2241930067539215\n",
      "Loss at step 75 = 0.22272710502147675\n",
      "Loss at step 76 = 0.21038083732128143\n",
      "Loss at step 77 = 0.22145476937294006\n",
      "Loss at step 78 = 0.2237522453069687\n",
      "Loss at step 79 = 0.20438282191753387\n",
      "Loss at step 80 = 0.24324895441532135\n",
      "Loss at step 81 = 0.2112320512533188\n",
      "Loss at step 82 = 0.23054943978786469\n",
      "Loss at step 83 = 0.20682036876678467\n",
      "Loss at step 84 = 0.2404392510652542\n",
      "Loss at step 85 = 0.217027485370636\n",
      "Loss at step 86 = 0.20037779211997986\n",
      "Loss at step 87 = 0.21171881258487701\n",
      "Loss at step 88 = 0.21251137554645538\n",
      "Loss at step 89 = 0.23529894649982452\n",
      "Loss at step 90 = 0.2223222553730011\n",
      "Loss at step 91 = 0.21515393257141113\n",
      "Loss at step 92 = 0.21746741235256195\n",
      "Loss at step 93 = 0.21751359105110168\n",
      "Loss at step 94 = 0.23313458263874054\n",
      "Loss at step 95 = 0.21781986951828003\n",
      "Loss at step 96 = 0.21650022268295288\n",
      "Loss at step 97 = 0.23100265860557556\n",
      "Loss at step 98 = 0.21108409762382507\n",
      "Loss at step 99 = 0.21085381507873535\n"
     ]
    }
   ],
   "source": [
    "nn.model.to(device)\n",
    "nn.model.train()\n",
    "\n",
    "print(f\"Training the neural network for {num_epochs} epochs\")\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    xs_shuffled = xs[shuffled_indices]\n",
    "    ys_shuffled = ys[shuffled_indices]\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        x = torch.FloatTensor(xs_shuffled[i : i + batch_size])\n",
    "        y = torch.FloatTensor(ys_shuffled[i : i + batch_size])\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = nn.model(x)\n",
    "        loss = criteria(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i == 0:\n",
    "            print(f\"Loss at step {epoch} = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0552eaea-8285-4efe-8cfc-1ea468bf9f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 0.21844428827241064\n",
      "Max/Min loss: 0.49200719594955444 , 0.12358696013689041\n"
     ]
    }
   ],
   "source": [
    "# Evalute the neural network\n",
    "nn.model.eval()\n",
    "losses = []\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    x = torch.FloatTensor(xs[i : i + batch_size])\n",
    "    y = torch.FloatTensor(ys[i : i + batch_size])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn.model(x)\n",
    "    loss = criteria(y_pred, y)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Mean loss: {np.mean(losses)}\")\n",
    "print(f\"Max/Min loss: {np.max(losses)} , {np.min(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05d88488-3a1c-41e5-ad31-1a6b4da5ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "print(\"Saving the model\")\n",
    "model_info = {\n",
    "    \"input_dims\": input_dims,\n",
    "    \"hidden_dims\": hidden_dims,\n",
    "    \"test_function\": save_name,\n",
    "    \"bounds\": bounds,\n",
    "    \"state_dict\": nn.model.state_dict(),\n",
    "}\n",
    "torch.save(\n",
    "    model_info,\n",
    "    f\"src/nn_models/nn_one_layer_{save_name}_{input_dims}_{hidden_dims}.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfbb8a-286e-447c-ad83-097df14d13b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jax]",
   "language": "python",
   "name": "conda-env-jax-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
